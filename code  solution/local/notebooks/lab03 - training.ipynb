{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d12b275-c8aa-412b-808c-fff0980b64d3",
   "metadata": {},
   "source": [
    "# Monitoring ML Training Pipeline: Model Training\n",
    "\n",
    "\n",
    "**Quick recap:**\n",
    "- Goal: \n",
    "    - Building a classication model for loan eligibility that predicts whether a loan is to be given or refused\n",
    "    - Introduce autonomous monitoring checkpoints orchestrated with Airflow DAGS\n",
    "- Download raw data: `raw/12196ecaa65e4831987aee4bfced5f60_2015-01-01_2015-05-31.csv`\n",
    "- Preprocessed the data into:\n",
    "    - training dataset: `preprocessed/12196ecaa65e4831987aee4bfced5f60.csv`\n",
    "    - test dataset: `preprocessed/12196ecaa65e4831987aee4bfced5f60.csv`\n",
    "\n",
    "**Next steps:**\n",
    "- load training and test datasets\n",
    "- check sanity\n",
    "- train multiple models: randomForest, gradientBoosting\n",
    "- select the best model\n",
    "    - auc >= 0.7\n",
    "    - abs(auc_train - auc_test) <=0.1\n",
    "- deploy model: compare the best model to last deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f312a289-ffb3-4ea9-94f3-b4f0b253c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connection to `database-4.cdnugfjpq15f.us-east-1.rds.amazonaws.com:loan_eligibility` initiated!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'd:\\\\Supriya projects\\\\ML Monitoring Camille\\\\main\\\\dags\\\\src\\\\config.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,  accuracy_score, f1_score, precision_score, recall_score, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from pprint import pprint\n",
    "from importlib import reload\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'dags', 'src'))\n",
    "\n",
    "import helpers\n",
    "import config\n",
    "\n",
    "reload(helpers)\n",
    "reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fec305c-f36d-488d-a284-6391b1c51508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### helpers.py methods ####\n",
    "def load_dataset(path:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data set.\n",
    "    :param path: str\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def check_dataset_sanity(df:pd.DataFrame) -> bool:\n",
    "    nulls = df.isnull().sum()\n",
    "    nulls = nulls[nulls>0].index.tolist()\n",
    "    if len(nulls)==0:\n",
    "        return True\n",
    "    else:\n",
    "        raise(Exception(f\"There are null values in the training dataset: {nulls}\"))\n",
    "\n",
    "def save_model_as_pickle(model, model_name, directory=None):\n",
    "    \"\"\"\n",
    "    Save a model as a pickle file.\n",
    "    :param model: AnyType\n",
    "    :param model_name: str\n",
    "    :param directory: str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if directory:\n",
    "        filename = os.path.join(directory, model_name+\".pkl\")\n",
    "    else:\n",
    "        filename = os.path.join(config.PATH_DIR_MODELS, model_name+\".pkl\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"[INFO] Model saved as pickle file:\", filename)\n",
    "\n",
    "def save_model_as_json(model:dict, model_name:str, directory:str=None):\n",
    "    \"\"\"\n",
    "    Save a model as a json file.\n",
    "    :param model: dict\n",
    "    :param model_name: str\n",
    "    :param directory: str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if directory:\n",
    "        filename = os.path.join(directory, model_name+\".json\")\n",
    "    else:\n",
    "        filename = os.path.join(config.PATH_DIR_MODELS, model_name+\".json\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(model, f)\n",
    "    print(\"[INFO] Model saved as json file:\", filename)\n",
    "\n",
    "def persist_deploy_report(job_id:str, model_name:str):\n",
    "    \"\"\"\n",
    "    Persist the deploy report of a job.\n",
    "    :param job_id: str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"job_id\": job_id,\n",
    "        \"purpose_to_int\": f\"{job_id}_purpose_to_int_model.json\",\n",
    "        \"missing_values\": f\"{job_id}_missing_values_model.pkl\",\n",
    "        \"prediction_model\": f\"{model_name}.pkl\",\n",
    "        \"train_report\": f\"{job_id}_train_report.json\",\n",
    "    }\n",
    "    json.dump(report, open(os.path.join(config.PATH_DIR_MODELS, f\"deploy_report.json\"), \"w\"))\n",
    "    print(f'[INFO] Deployment report saved as {os.path.join(config.PATH_DIR_MODELS, f\"deploy_report.json\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fbcd940-0971-48dc-995f-9fca67ee0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train.py methods #####\n",
    "def train(train_dataset_filename:str=None, test_dataset_filename:str=None, job_id=\"\", rescale=False):\n",
    "    \"\"\"\n",
    "    Train a model on the train dataset loaded from `train_dataset_filename` and test dataset loaded from `test_dataset_filename`\n",
    "    :param train_dataset_filename: str\n",
    "    :param test_dataset_filename: str\n",
    "    :param job_id: str\n",
    "    :param rescale: bool, if true, scaled numerical variables used\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if train_dataset_filename==None:\n",
    "        train_dataset_filename = os.path.join(config.PATH_DIR_DATA, \"preprocessed\", f\"{job_id}_training.csv\")\n",
    "    if test_dataset_filename==None:\n",
    "        test_dataset_filename = os.path.join(config.PATH_DIR_DATA, \"preprocessed\", f\"{job_id}_inference.csv\")\n",
    "    tdf = helpers.load_dataset(train_dataset_filename)\n",
    "    vdf = helpers.load_dataset(test_dataset_filename)\n",
    "    helpers.check_dataset_sanity(tdf)\n",
    "    helpers.check_dataset_sanity(vdf)\n",
    "    \n",
    "    predictors = config.PREDICTORS\n",
    "    target = config.TARGET\n",
    "    if rescale:\n",
    "        for col in predictors:\n",
    "            if f\"{config.RESCALE_METHOD}_{col}\" in tdf.columns:\n",
    "                tdf[col] = tdf[f\"{config.RESCALE_METHOD}_{col}\"]\n",
    "            if f\"{config.RESCALE_METHOD}_{col}\" in vdf.columns:\n",
    "                vdf[col] = vdf[f\"{config.RESCALE_METHOD}_{col}\"]\n",
    "        \n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=config.RANDOM_SEED)\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, max_depth=10, random_state=config.RANDOM_SEED)\n",
    "    X, Y = tdf[predictors], tdf[target]\n",
    "    report = dict()\n",
    "    models = dict()\n",
    "    for cl, name in [(rf, \"rf\"), (gb, \"gb\")]:\n",
    "        print(\"[INFO] Training model:\", name)\n",
    "        cl.fit(X, Y)\n",
    "        t_pred = cl.predict(X)\n",
    "        v_pred = cl.predict(vdf[predictors])\n",
    "        t_prob = cl.predict_proba(X)[:, 1]\n",
    "        v_prob = cl.predict_proba(vdf[predictors])[:, 1]\n",
    "        report[f\"{name}_train\"] = performance_report(Y, t_pred, t_prob)\n",
    "        report[f\"{name}_test\"] = performance_report(vdf[target], v_pred, v_prob)\n",
    "        models[name] = cl\n",
    "        \n",
    "    model_name = select_model(pd.DataFrame(report), metric=config.MODEL_PERFORMANCE_METRIC, model_names=list(models.keys()))\n",
    "    report[\"final_model\"] = model_name\n",
    "    helpers.save_model_as_pickle(models[model_name], f\"{job_id}_{model_name}\")\n",
    "    helpers.save_model_as_json(report, f\"{job_id}_train_report\")\n",
    "    return report\n",
    "\n",
    "def performance_report(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Generate performance report for a model.\n",
    "    :param y_true: np.array, true value\n",
    "    :param y_pred: np.array, predicted values\n",
    "    :param y_prob: np.array, prediction probability\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    report = dict()\n",
    "    report[\"dataset size\"] = y_true.shape[0]\n",
    "    report[\"positive rate\"] = y_true.sum()/y_true.shape[0]\n",
    "    report[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    report[\"f1\"] = f1_score(y_true, y_pred)\n",
    "    report[\"precision\"] = precision_score(y_true, y_pred)\n",
    "    report[\"recall\"] = recall_score(y_true, y_pred)\n",
    "    report[\"auc\"] = roc_auc_score(y_true, y_prob)\n",
    "    return report\n",
    "\n",
    "def select_model(df:pd.DataFrame, metric:str=config.MODEL_PERFORMANCE_METRIC, model_names:list=[\"rf\", \"gb\"], performance_thresh:float=config.MODEL_PERFORMANCE_THRESHOLD, degradation_thresh:float=config.MODEL_DEGRADATION_THRESHOLD)->str:\n",
    "    \"\"\"\n",
    "    Select the best model based on their performance reports.\n",
    "        - metric >= performance_thresh where metric can be auc, recall, precision, f1_score, ... and performance_thresh is any value between 0.0 and 1.0\n",
    "        - abs(<metric>_train - <metric>_test) <= degradation_thresh\n",
    "    :param df: pd.DataFrame, performance report\n",
    "    :param metric: str, metric to select the best model.\n",
    "    :param model_names: list, model names to select from.\n",
    "    :param performance_thresh: float, threshold for the performance.\n",
    "    :return: str, model name.\n",
    "    \"\"\"\n",
    "    degradation_performance = []\n",
    "    for model in model_names:\n",
    "        if df.loc[metric, f\"{model}_train\"] < performance_thresh:\n",
    "            continue\n",
    "        degradation = df.loc[metric, f\"{model}_train\"] - df.loc[metric, f\"{model}_test\"]\n",
    "        if degradation < degradation_thresh:\n",
    "            degradation_performance.append((model, degradation))\n",
    "    if len(degradation_performance) == 0:\n",
    "        raise(Exception(\"No model selected: all models have performance below the threshold. Possible overfitting.\"))\n",
    "    return min(degradation_performance, key=lambda x: x[1])[0]\n",
    "\n",
    "def pick_model_and_deploy(job_id, models, df, metric=\"auc\", predictors=config.PREDICTORS, target=config.TARGET)->str:\n",
    "    \"\"\"\n",
    "    Among all `models`, select the model that performs best on df and mark it for deployment.\n",
    "    :param job_id: str, job id.\n",
    "    :param models: list of key-value items {\"job_id\": <str>, \"purpose_to_int: <str>, \"missing_values\": <str>, \"prediction_model\": <>, \"train_report\": <str>}\n",
    "    :param df: pd.DataFrame, test dataset\n",
    "    :param metric: str, metric used to select the best model.\n",
    "    :param predictors: list, predictors to use.\n",
    "    :param target: str, target to use.\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    assert len(models) > 0, \"`models` cannot be empty\"\n",
    "    if len(models)==1:\n",
    "        model_name = models[0][\"model_name\"]\n",
    "        helpers.persist_deploy_report(job_id, model_name)\n",
    "        return model_name\n",
    "    cols = set(predictors).difference(set(df.columns))\n",
    "    assert cols == set(), f\"{cols} not in {df.columns}\"\n",
    "    score = 0\n",
    "    m_idx = 0\n",
    "    for i, m in enumerate(models):\n",
    "        y_true = df[target]\n",
    "        y_pred = m[\"model\"].predict(df[predictors])\n",
    "        y_prob = m[\"model\"].predict_proba(df[predictors])[:, 1]\n",
    "        r = performance_report(y_true, y_pred, y_prob)\n",
    "        if r[metric] > score:\n",
    "            score = r[metric]\n",
    "            m_idx = i\n",
    "    helpers.persist_deploy_report(job_id, models[m_idx][\"model_name\"])\n",
    "    return models[m_idx][\"model_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab7d2fd2-713a-4187-a952-48c912e9e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model: rf\n",
      "[INFO] Training model: gb\n",
      "[INFO] Model saved as pickle file: ../dags/models\\12196ecaa65e4831987aee4bfced5f60_rf.pkl\n",
      "[INFO] Model saved as json file: ../dags/models\\12196ecaa65e4831987aee4bfced5f60_train_report.json\n",
      "{'final_model': 'rf',\n",
      " 'gb_test': {'accuracy': 0.8914473684210527,\n",
      "             'auc': 0.9324188014866938,\n",
      "             'dataset size': 6688,\n",
      "             'f1': 0.9321875583784792,\n",
      "             'positive rate': 0.7764653110047847,\n",
      "             'precision': 0.9051333212407038,\n",
      "             'recall': 0.9609089158482572},\n",
      " 'gb_train': {'accuracy': 0.9961346960167715,\n",
      "              'auc': 0.9999797096398029,\n",
      "              'dataset size': 15264,\n",
      "              'f1': 0.9974556901979387,\n",
      "              'positive rate': 0.7576650943396226,\n",
      "              'precision': 0.9949242945629732,\n",
      "              'recall': 1.0},\n",
      " 'rf_test': {'accuracy': 0.9020633971291866,\n",
      "             'auc': 0.9377489764649738,\n",
      "             'dataset size': 6688,\n",
      "             'f1': 0.9394471664971803,\n",
      "             'positive rate': 0.7764653110047847,\n",
      "             'precision': 0.9034495021337127,\n",
      "             'recall': 0.9784325052955902},\n",
      " 'rf_train': {'accuracy': 0.8986504192872118,\n",
      "              'auc': 0.9669825347451965,\n",
      "              'dataset size': 15264,\n",
      "              'f1': 0.9362140766090793,\n",
      "              'positive rate': 0.7576650943396226,\n",
      "              'precision': 0.894782471626734,\n",
      "              'recall': 0.9816688283614353}}\n"
     ]
    }
   ],
   "source": [
    "job_id = \"12196ecaa65e4831987aee4bfced5f60\"\n",
    "report = train(job_id=\"12196ecaa65e4831987aee4bfced5f60\")\n",
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d72ca82-acd7-494e-a583-5c2c9d73583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deployment report saved as ../dags/models\\deploy_report.json\n",
      "Deployed model: 12196ecaa65e4831987aee4bfced5f60_rf\n"
     ]
    }
   ],
   "source": [
    "model = pick_model_and_deploy(\n",
    "    job_id=job_id,\n",
    "    df = pd.read_csv(\"../dags/data/preprocessed/12196ecaa65e4831987aee4bfced5f60_inference.csv\"),\n",
    "    models = [{\n",
    "        \"model_name\": f\"{job_id}_{report['final_model']}\", \n",
    "        \"model\": pickle.load(open(\"../dags/models/12196ecaa65e4831987aee4bfced5f60_rf.pkl\", \"rb\"))\n",
    "    }]\n",
    ")\n",
    "print(\"Deployed model:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e4e3451-5fa3-423d-a773-1994543854a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deloyment Report Sample\n",
      "{'job_id': '12196ecaa65e4831987aee4bfced5f60',\n",
      " 'missing_values': '12196ecaa65e4831987aee4bfced5f60_missing_values_model.pkl',\n",
      " 'prediction_model': '12196ecaa65e4831987aee4bfced5f60_rf.pkl',\n",
      " 'purpose_to_int': '12196ecaa65e4831987aee4bfced5f60_purpose_to_int_model.json',\n",
      " 'train_report': '12196ecaa65e4831987aee4bfced5f60_train_report.json'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Deloyment Report Sample\")\n",
    "pprint(json.load(open(\"../dags/models/deploy_report.json\", \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9b15f-52fe-4c07-809d-fead41879fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('Anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9054e5812adb29eebbcd6b680e8ef1afc4fe6e00a75ff130e735bd95b5b32301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
