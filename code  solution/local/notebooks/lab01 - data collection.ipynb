{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69a2b59-3729-4285-b6aa-ef2b77a7556d",
   "metadata": {},
   "source": [
    "# Monitoring ML Training Pipeline: Data Extraction\n",
    "\n",
    "**This notebook will extract data from postgresql and store it in your local system**\n",
    "\n",
    "**Skip this if you do not have data stored in postgres and use preloaded data in the data folder**\n",
    "\n",
    "**Objective** : Building a classication model for loan eligibility that predicts whether a loan is to be given or refused\n",
    "\n",
    "**Data**\n",
    "\n",
    "![customer_details](../images/table_customer_details.png)\n",
    "\n",
    "![credit_details](../images/table_credit_details.png)\n",
    "\n",
    "![loan_details](../images/table_loan_details.png)\n",
    "\n",
    "**Data quality challenges:**\n",
    "- Duplication\n",
    "- Date format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cddf84-725d-4034-82cb-85a5b2907728",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-requisits tools\n",
    "- Postgres\n",
    "- sqlalchemy\n",
    "## Queries\n",
    "All queries are stored in a single script: `src/queries.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca05f161-b569-4511-9885-5a8bdd84dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_TEMP_TABLE_LOAN = \"\"\"\n",
    "    create temp table loan as (\n",
    "        select \n",
    "            lower(t1.loan_id) loan_id,\n",
    "            lower(t1.customer_id) customer_id,\n",
    "            lower(t1.loan_status) loan_status,\n",
    "            cast(concat(split_part(t1.application_time, '-', 2), '-', split_part(t1.application_time, '-', 1), '-', split_part(t1.application_time, '-', 3)) as timestamp) application_time,\n",
    "            t1.current_loan_amount,\n",
    "            lower(t1.term) term,\n",
    "            t1.tax_liens,\n",
    "            lower(t1.purpose) purpose,\n",
    "            t1.no_of_properties \n",
    "        from (\n",
    "            select \n",
    "                row_number() over(partition by loan_id order by application_time desc) rnk,\n",
    "                ld.*\n",
    "            from (\n",
    "                select distinct * \n",
    "                from loan_details \n",
    "                where cast(concat(split_part(application_time, '-', 2), '-', split_part(application_time, '-', 1), '-', split_part(application_time, '-', 3)) as timestamp) between '{start_date}' and '{end_date}') ld\n",
    "        ) t1\n",
    "        where rnk=1\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "CREATE_TEMP_TABLE_CUSTOMER = \"\"\"\n",
    "    create temp table customer as (\n",
    "        select t2.* \n",
    "        from (\n",
    "            select customer_id, count(*) cnt from (select distinct * from customer_details) cd \n",
    "            group by customer_id \n",
    "        ) t1 \n",
    "        join (select distinct * from customer_details) t2\n",
    "        on t2.customer_id = t1.customer_id\n",
    "        where t1.cnt=1\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "CREATE_TEMP_TABLE_CREDIT = \"\"\"\n",
    "    create temp table credit as (\n",
    "        select t2.* \n",
    "        from (\n",
    "            select customer_id, count(*) cnt from (select distinct * from credit_details) cd \n",
    "            group by customer_id \n",
    "        ) t1 \n",
    "        join (select distinct * from credit_details) t2\n",
    "        on t2.customer_id = t1.customer_id\n",
    "        where t1.cnt=1\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "GET_DATA = \"\"\"\n",
    "    select \n",
    "        t1.loan_id, t1.customer_id, t1.loan_status, t1.application_time, t1.current_loan_amount, t1.term, t1.tax_liens, t1.purpose, t1.no_of_properties,\n",
    "        lower(t2.home_ownership) home_ownership, t2.annual_income, lower(t2.years_in_current_job) years_in_current_job, t2.months_since_last_delinquent, t2.no_of_cars, t2.no_of_children,\n",
    "        t3.credit_score, t3.monthly_debt, t3.years_of_credit_history, t3.no_of_open_accounts, t3.no_of_credit_problems, t3.current_credit_balance, t3.max_open_credit, t3.bankruptcies\n",
    "    from loan t1\n",
    "    left join customer t2\n",
    "    on t2.customer_id = t1.customer_id\n",
    "    left join credit t3\n",
    "    on t3.customer_id = t2.customer_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fed0d-1bc4-4c55-a9c8-726a8caece57",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "1. Create connection to the data -> available in script `src/helpers.py`\n",
    "2. Create temporarly tables -> available in script `src/etl.py`\n",
    "3. Get the data from joined temporarly tables -> available in script `src/etl.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda0611a-4923-45c6-a472-4abbcec697ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhaga\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\bhaga\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\bhaga\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from importlib import reload\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'dags', 'src'))\n",
    "\n",
    "#make sure you have SQL Credentials.json stored in dags folder\n",
    "import helpers\n",
    "import config\n",
    "import queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9437b9-19ad-4f58-8efc-265ab1b8c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### helpers.py methods ####\n",
    "def save_dataset(df:pd.DataFrame, path:str):\n",
    "    \"\"\"\n",
    "    Save data set.\n",
    "    :param df: DataFrame\n",
    "    :param path: str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[INFO] Dataset saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb98e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.3-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654502a9-80ae-45e0-bbe2-61abe2278939",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = json.load(open(config.PATH_TO_CREDENTIALS, 'r'))\n",
    "engine = create_engine(f\"postgresql://{credentials['user']}:{credentials['password']}@{credentials['host']}:{credentials['port']}/{credentials['database']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36794cc4-dd1b-472e-a7e7-a416bbdb5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(start_date:datetime.date, end_date:datetime.date=datetime.date.today()) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from the database and returns it as a pandas dataframe.\n",
    "    Queries are to be defined in the `queries.py` file.\n",
    "    :param start_date: start date of the data to be extracted\n",
    "    :param end_date: end date of the data to be extracted\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    assert start_date <= end_date, \"start_date must be less than end_date\"\n",
    "    print(\"[INFO] Extracting data from the database since {0} to {1} ...\".format(start_date, end_date))\n",
    "    engine.execute(text(\"\"\"drop table if exists customer;\"\"\").execution_options(autocommit=True))\n",
    "    engine.execute(text(queries.CREATE_TEMP_TABLE_CUSTOMER).execution_options(autocommit=True))\n",
    "    engine.execute(text(\"\"\"drop table if exists loan;\"\"\").execution_options(autocommit=True))\n",
    "    engine.execute(text(queries.CREATE_TEMP_TABLE_LOAN.format(start_date=start_date, end_date=end_date)).execution_options(autocommit=True))\n",
    "    engine.execute(text(\"\"\"drop table if exists credit;\"\"\").execution_options(autocommit=True))\n",
    "    engine.execute(text(queries.CREATE_TEMP_TABLE_CREDIT).execution_options(autocommit=True))\n",
    "    df = pd.read_sql(text(queries.GET_DATA), engine)\n",
    "    return df\n",
    "\n",
    "def collect_data(start_date:datetime.date, end_date:datetime.date=datetime.date.today(), job_id:str=None):\n",
    "    \"\"\"\n",
    "    Collects data from the database and dump them in the directory of raw data `config.PATH_DIR_DATA`.\n",
    "    :param start_date: start date of the data to be extracted\n",
    "    :param end_date: end date of the data to be extracted\n",
    "    :param job_id: job id of the data to be extracted\n",
    "    \"\"\"\n",
    "    assert isinstance(start_date, datetime.date)\n",
    "    assert isinstance(end_date, datetime.date)\n",
    "    assert isinstance(job_id, str)\n",
    "    assert start_date <= end_date\n",
    "    size = 0\n",
    "\n",
    "    df = extract_data(start_date, end_date)\n",
    "    size = df.shape[0]\n",
    "    filename = os.path.join(config.PATH_DIR_DATA, \"raw\", f\"{job_id}_\"+start_date.strftime(\"%Y-%m-%d\")+\"_\"+end_date.strftime(\"%Y-%m-%d\")+\".csv\")\n",
    "    helpers.save_dataset(df, filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6a7fd4-ba15-418f-892c-0721847a40f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job Id: aa4c3eaadb02409281b589829e3c9370\n",
      "[INFO] Extracting data from the database since 2015-06-01 to 2015-12-31 ...\n",
      "[INFO] Dataset saved to ../dags/data\\raw\\aa4c3eaadb02409281b589829e3c9370_2015-06-01_2015-12-31.csv\n",
      "../dags/data\\raw\\aa4c3eaadb02409281b589829e3c9370_2015-06-01_2015-12-31.csv\n"
     ]
    }
   ],
   "source": [
    "job_id = helpers.generate_uuid()\n",
    "start_date = datetime.date(2015, 1, 1)\n",
    "end_date = datetime.date(2015, 5, 31)\n",
    "print(\"job Id:\", job_id)\n",
    "print(collect_data(start_date, end_date, job_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7a048-ee33-42b4-9e24-c9330da7103c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259adcf3-e72a-4456-8cd8-e32a77c2bb42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('Anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9054e5812adb29eebbcd6b680e8ef1afc4fe6e00a75ff130e735bd95b5b32301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
