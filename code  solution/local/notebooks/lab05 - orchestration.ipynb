{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ab74f4-7d6b-437e-84df-4579fc79d6e5",
   "metadata": {},
   "source": [
    "# Monitoring ML Training Pipeline: Orchestration\n",
    "\n",
    "**Quick recap:**\n",
    "- Goal: \n",
    "    - Building a classication model for loan eligibility that predicts whether a loan is to be given or refused\n",
    "    - monitor data drifts and model drifts\n",
    "- Download raw data: `raw/12196ecaa65e4831987aee4bfced5f60_2015-01-01_2015-05-31.csv`\n",
    "- Preprocessed the data into:\n",
    "    - training dataset: `preprocessed/12196ecaa65e4831987aee4bfced5f60.csv`\n",
    "    - test dataset: `preprocessed/12196ecaa65e4831987aee4bfced5f60.csv`\n",
    "\n",
    "- Trained and deployed the model\n",
    "    - JobID: 12196ecaa65e4831987aee4bfced5f60\n",
    "    - Missing values: 12196ecaa65e4831987aee4bfced5f60_missing_values_model.pkl\n",
    "    - Purpose to Integer: 12196ecaa65e4831987aee4bfced5f60_purpose_to_int_model.json\n",
    "    - Prediction model: 12196ecaa65e4831987aee4bfced5f60_rf.pkl\n",
    "\n",
    "**Next steps:**\n",
    "- orchestration of the monitoring steps and process\n",
    "    - Install Docker\n",
    "    - Setup Airflow in a docker container\n",
    "    - Create an run Airflow DAG\n",
    "    - Integrate Airflow DAG with Slack\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/monitored_pipeline_slack_integration.png\" width=\"850\"/>\n",
    "</div>\n",
    "\n",
    "## I. [Install Docker](https://docs.docker.com/get-docker/)\n",
    "## II. Deploy a docker container for Airflow\n",
    "- Folder structuring\n",
    "```\n",
    "├── dags\n",
    "│   ├── data\n",
    "│   │   ├── preprocessed\n",
    "│   │   └── raw\n",
    "│   ├── models\n",
    "│   ├── results\n",
    "│   └── src\n",
    "docker-compose.yaml\n",
    "├── jobs\n",
    "├── logs\n",
    "└── plugins\n",
    "```\n",
    "- Services\n",
    "- UI\n",
    "## III. Prepare slack for integration with Airflow\n",
    "- Create slack app and generate token: https://api.slack.com/apps\n",
    "- Create slack connection in Airflow\n",
    "- Test Airflow-Slack integration\n",
    "## IV. Create training dag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff53f5c-ee1a-4705-9b66-a9a2260840cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##### queries.py #####\n",
    "CREATE_TABLE_ML_JOB = \"\"\"\n",
    "create table if not exists mljob (\n",
    "    id serial primary key,\n",
    "    job_id varchar(36) not null,\n",
    "    job_type varchar(36), -- training, inference\n",
    "    job_date date, \n",
    "    stage varchar(36) not null, -- etl, preprocess, training, deploy, predicting\n",
    "    status varchar(36) not null, -- started, pass, failed\n",
    "    message text not null, -- e.g: exceptions\n",
    "    created_at timestamp not null default now()\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "LOG_ACTIVITY = \"\"\"\n",
    "    insert into mljob (\n",
    "        job_id,\n",
    "        job_type,\n",
    "        job_date,\n",
    "        stage,\n",
    "        status,\n",
    "        message\n",
    "    ) values ('{job_id}', '{job_type}', '{job_date}', '{stage}', '{status}', '{message}')\n",
    "\"\"\"\n",
    "\n",
    "GET_LATEST_DEPLOYED_JOB_ID = \"\"\"\n",
    "    select job_id from mljob\n",
    "    where status = '{status}' and job_type = 'training' and stage = 'deploy'\n",
    "    order by created_at desc\n",
    "    limit 1\n",
    "\"\"\"\n",
    "\n",
    "##### helpers.py #####\n",
    "# ---> job handlers\n",
    "def generate_uuid() -> str:\n",
    "    \"\"\"\n",
    "    Generate a random UUID.\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    return str(uuid.uuid4()).replace(\"-\", \"\")\n",
    "\n",
    "# ---> db handlers\n",
    "def log_activity(job_id:str, job_type:str, stage:str, status:str, message:str, job_date:datetime.date=None):\n",
    "    \"\"\"\n",
    "    Logs the activity of a job.\n",
    "    :param job_id: str\n",
    "    :param job_type: str\n",
    "    :param stage: str\n",
    "    :param status: str\n",
    "    :param message: str\n",
    "    :param job_date: datetime.date\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    assert stage in config.STAGES, f\"[ERROR] Stage `{stage}` is not valid! Choose from {config.STAGES}\"\n",
    "    assert status in config.STATUS, f\"[ERROR] Status `{status}` is not valid! Choose from {config.STATUS}\"\n",
    "    assert job_type in config.JOB_TYPES, f\"[ERROR] Job type `{job_type}` is not valid! Choose from {config.JOB_TYPES}\"\n",
    "    message = message.replace(\"'\", \"\\\\\")\n",
    "    engine.execute(text(queries.LOG_ACTIVITY.format(job_id=str(job_id), job_type=job_type, stage=str(stage), status=str(status), message=message, job_date=job_date)).execution_options(autocommit=True))\n",
    "    print(f\"[INFO] Job {job_id} logged as {job_type}::{stage}::{status}::{message}\")\n",
    "\n",
    "    \n",
    "def get_latest_deployed_job_id(status:str=\"pass\") -> str:\n",
    "    \"\"\"\n",
    "    Get the latest deployed job id by looking for the latest of all jobs with stage `deploy` and the specified status.\n",
    "    :param status: str\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.load(open(os.path.join(config.PATH_DIR_MODELS, \"deploy_report.json\"))).get(\"job_id\")\n",
    "    except Exception as e:\n",
    "        assert status in config.STATUS, f\"[ERROR] Status `{status}` is not valid! Choose from {config.STATUS}\"\n",
    "        query = text(queries.GET_LATEST_DEPLOYED_JOB_ID.format(status=status))\n",
    "        r = pd.read_sql(query, engine)\n",
    "        if r.shape[0] == 0:\n",
    "            return None\n",
    "        return str(r['job_id'].values[0])\n",
    "\n",
    "# ---> files handlers\n",
    "def load_dataset(path:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data set.\n",
    "    :param path: str\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def locate_raw_data_filename(job_id:str) -> str:\n",
    "    \"\"\"\n",
    "    Locate the raw data file.\n",
    "    :param job_id: str\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob(os.path.join(config.PATH_DIR_DATA, \"raw\", f\"{job_id}_*.csv\"))\n",
    "    if len(files) == 0:\n",
    "        print(f\"[WARNING] No raw data file found for job_id : {job_id}.\")\n",
    "        return None\n",
    "    return files[0]\n",
    "\n",
    "def locate_preprocessed_filenames(job_id:str) -> dict:\n",
    "    \"\"\"\n",
    "    Locate the preprocessed data files.\n",
    "    :param job_id: str\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    files = sorted(glob(os.path.join(config.PATH_DIR_DATA, \"preprocessed\", f\"{job_id}_*.csv\")))\n",
    "    if len(files) == 0:\n",
    "        raise(Exception(f\"No preprocessed data file found for job_id : {job_id}.\"))\n",
    "    elif len(files) > 2:\n",
    "        raise(Exception(f\"More than one preprocessed data file found for job_id : {job_id} ->\\n{files}\"))\n",
    "    elif len(files) == 1:\n",
    "        training_filename = None\n",
    "        inference_filename = list(filter(lambda x: \"inference\" in x, files))[0]\n",
    "        return training_filename, inference_filename\n",
    "    else:\n",
    "        training_filename = list(filter(lambda x: \"training\" in x, files))[0]\n",
    "        inference_filename = list(filter(lambda x: \"inference\" in x, files))[0]\n",
    "        return training_filename, inference_filename\n",
    "\n",
    "def get_model_type(job_id:str) -> str:\n",
    "    \"\"\"\n",
    "    Get the type of a model.\n",
    "    :param job_id: str\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    report_filename = os.path.join(config.PATH_DIR_MODELS, f\"{job_id}_train_report.json\")\n",
    "    return json.load(open(report_filename, \"r\"))[\"final_model\"]\n",
    "\n",
    "def load_model_from_pickle(model_name: str):\n",
    "    \"\"\"\n",
    "    Load a pickle model.\n",
    "    :param model_name: str\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    with open(os.path.join(config.PATH_DIR_MODELS, model_name+\".pkl\"), \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ba0aa-20d2-4301-b68b-0afbe27fc78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('Anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9054e5812adb29eebbcd6b680e8ef1afc4fe6e00a75ff130e735bd95b5b32301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
